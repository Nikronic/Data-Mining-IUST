{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Mining - HW01 - 98722278\n",
    "\n",
    "Index:\n",
    "1. Let's assume you work as a data mining consultant in a company which workes on search engines. Explain how using\n",
    "techniques such as *Associate Rules, Clustering, Classification and Anomaly Detection* may help you.\n",
    "2. Compute *Cosine distance, Correlation, Euclidean Distance and Jacard Index* for these two pair of vectors:\n",
    "    1. `V1 = [1, 1, 1, 1]`, `V2 = [2, 2, 2, 2]`\n",
    "    2. `V1 = [1, 0, 1, 0]`, `V2 = [0, 1, 0, 1]`\n",
    "3. Consider finding K-nearest neighbors of a dataset. Here is the proposed algorithm:\n",
    "    1. What happens if there is duplicate entries in dataset? (consider the distance of duplicate entries 0)\n",
    "    2. How to solve this issue?\n",
    "\n",
    "![proposed alg](wiki\\3.jpg)\n",
    "\n",
    "4. Compare dimentionality reduction approaches such as *PCA and SVD* versus *Aggregation based* methods.\n",
    "\n",
    "5. What are pros and cons of sampling for reducing number of samples? Is sampling without replacement is a good\n",
    "approach?\n",
    "\n",
    "6. Download [Telco Churn dataset](https://www.kaggle.com/blastchar/telco-customer-churn)\n",
    "    1. Analyze dataset\n",
    "    2. Implement *Apriori* algorithm from scratch. Regarding aforementioned dataset, find proper parameteres. By\n",
    "    increasing and decreasing different parameters such as *confidence* and *support* report the changes.\n",
    "    3. As the output, report finest extract rules\n",
    "\n",
    "\n",
    " 7. In this section, the goal is to train *KNN* and *DecisionTree* on [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)\n",
    "    1. Preprocess\n",
    "    2. Split into Train, test and Validation\n",
    "    3. Train models\n",
    "        1. KNN\n",
    "        2. DecisionTree\n",
    "    4. Report accuracy, recall and F-score metrics\n",
    "        1. KNN\n",
    "        2. DecisionTree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 Classification, Clustering, Rules, Anomaly, etc in Search Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The most important feature of search engines is that they must provide result based on their relevance. But to\n",
    "achieve these ranking for millions of possible outputs, search engines collect data and apply different machine\n",
    "learning methods to achieve some score for each possible result or any combination of them. Let's go throughout the\n",
    "some of methods such as Classification, Clustering, etc to clarify the situation.\n",
    "\n",
    "**Classification** enables the machine to differetiate between different classes of contents. For instace, you looking\n",
    "for a laptop for you needs, so the machine should be able to classify the content on the websites into different\n",
    "categories and show you the results that only contain the classes you looking for. As we said there are billion of\n",
    "websites out there so by just classifying the entire data into few categories (your keywords) the search space will\n",
    "be much smaller to analyze for other steps.\n",
    "\n",
    "Now let's consider the effect of **Association Rules**. Consider previous example which you wanted to buy a laptop.\n",
    "It is a general case and after searching it you might not find wat you were looking for. Search engine needs more\n",
    "keyword to specificly find your interests. So you add a laptop for teaching purposes. So, by adding this keyword,\n",
    "search engines knows some rules that people who bought these type of laptops, also might need microphone and a good\n",
    "webcam too. So if you try to search for mic or camera, you will get mostly results related to match with the\n",
    "requirement of having a lecture.\n",
    "\n",
    "But what is **Anomaly Detection**? In previous example, you are a student and looking for a mid-range devices because\n",
    " you need only a mediocre voice quality plus simple webcam to just communicate better. Mostly you are engaged with\n",
    " slides rather than your face! Anomaly means something that is not usual within results. In our case, most of the\n",
    " population who want laptop, mic and camera might be students or university lectureres, but gamers are here too. They\n",
    "  need much more powerfull mic and cameras. So if you are student, you are not looking for a 2K resolution! Here the\n",
    "  population wit less quantity is anomaly and need to be considered with different rules. Another example for this\n",
    "  can be the typo in another languages. For instance, in middle eastern countries, most of the people forget to\n",
    "  switch language to english and write their keywords in Arabic script. In this case a undefined word is being used\n",
    "  enormously only for few IP ranges. So it triggers and alert and make the search engine to learn that anomalous\n",
    "  behavior and convert it to a feature!\n",
    "\n",
    "  **Clustering** is similar to classification but with this difference that the distance metric is the core of\n",
    "  clustering which enables us to rank different results based on their distances to different center of clusters.\n",
    "  Clusters provide topic-based results which also can be incorporated within an another cluster where enables\n",
    "  hierarchical understanding of different topics and catergories. Clustering can be used for group of people in a\n",
    "  particular location, for instance, people in middle east search for different clothes in summer than people in\n",
    "  Russia. Or another case would be you looking for a specific keyword but it is rare or you don't enjoy the result,\n",
    "  then search engine use clusters and replace the word you used by a more related word using underlying meaning of\n",
    "  the keywords based on the clusters they are near to.\n",
    "\n",
    "  PPS:\n",
    "  1. Note that all of the previously mentioned approaches can be combined and even can be embedded within each\n",
    "  other to\n",
    "  provide more robust algorithms.\n",
    "  2. Also, I have mostly used \"word\" and \"keyword\" as the input. Same definitions can be used for all type of inputs\n",
    "  such as music or images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2 *Cosine distance, Correlation, Euclidean Distance and Jacard Index* for\n",
    "1. `V1 = [1, 1, 1, 1]`, `V2 = [2, 2, 2, 2]`\n",
    "2. `V1 = [1, 0, 1, 0]`, `V2 = [0, 1, 0, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_distance(v1, v2):\n",
    "    \"\"\"\n",
    "    Computes the cosine distance between two ND vectors\n",
    "    s = V1.V2 / (|A|.|B|)\n",
    "\n",
    "    :param v1: First vector\n",
    "    :param v2: Second Vector\n",
    "    :return: A float number\n",
    "    \"\"\"\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "\n",
    "    dot_product = np.sum([v1_ * v2_ for v1_, v2_ in zip(v1, v2)])\n",
    "    return dot_product / (np.sqrt(np.sum(v1 ** 2)) * np.sqrt(np.sum(v2 ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def correlation(v1, v2):\n",
    "    \"\"\"\n",
    "    Computes the correlation between two ND vectors\n",
    "    s = ((v1- v1_mean).(v2-v2_mean)) / sqrt((v1- v1_mean)**2.(v2-v2_mean)**2)\n",
    "\n",
    "    :param v1: First vector\n",
    "    :param v2: Second Vector\n",
    "    :return: A float number\n",
    "    \"\"\"\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "\n",
    "    v1_norm = v1 - np.mean(v1)\n",
    "    v2_norm = v2 - np.mean(v2)\n",
    "\n",
    "    cov = np.sum([v1_ * v2_ for v1_, v2_ in zip(v1_norm, v2_norm)])\n",
    "    denom = np.sqrt(np.sum(v1_norm**2) * np.sum(v2_norm**2))\n",
    "    return  cov / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(v1, v2):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the correlation between two ND vectors\n",
    "    s = sum((v1-v2)**2)\n",
    "\n",
    "    :param v1: First vector\n",
    "    :param v2: Second Vector\n",
    "    :return: A float number\n",
    "    \"\"\"\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "\n",
    "    return np.sqrt(np.sum([(v1_ - v2_)**2 for v1_, v2_ in zip(v1, v2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard_index(v1, v2):\n",
    "    \"\"\"\n",
    "    Computes the Jaccard index between two ND vectors\n",
    "    s = intersection(v1, v2) / union(v1, v2)\n",
    "\n",
    "    :param v1: First vector\n",
    "    :param v2: Second Vector\n",
    "    :return: A float number\n",
    "    \"\"\"\n",
    "    m11 = np.sum(v1 & v2)\n",
    "    m01 = np.sum(~v1 & v2)\n",
    "    m10 = np.sum(v1 & ~v2)\n",
    "    return m11 / (m11 + m01 + m10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine=1.0, Correlation=nan, Eculidean=2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nikan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([1, 1, 1, 1])\n",
    "v2 = np.array([2, 2, 2, 2])\n",
    "print('Cosine={}, Correlation={}, Eculidean={}'.format(cosine_distance(v1, v2), correlation(v1, v2), euclidean_distance(v1, v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine=0.0, Correlation=-1.0, Eculidean=2.0, Jaccard=0.0\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([1, 0, 1, 0])\n",
    "v2 = np.array([0, 1, 0, 1])\n",
    "\n",
    "print('Cosine={}, Correlation={}, Eculidean={}, Jaccard={}'.format(cosine_distance(v1, v2), correlation(v1, v2),\n",
    "                                                                   euclidean_distance(v1, v2), jaccard_index(v1, v2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3 K-Nearest Neighbors:\n",
    "1. What happens if there is duplicate entries in dataset? (consider the distance of duplicate entries 0)\n",
    "2. How to solve this issue?\n",
    "\n",
    "![proposed alg](wiki\\3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.A Duplicate entries\n",
    "If we have duplicate entries which means zero distance in our distance transformed matrix of data, then for each\n",
    "entry `i` if we have for instance `j` duplicates, then `i`'s top `j`s anwer would be only duplicate information so in\n",
    " KNN algorithm `j`th items out of `k` will be duplicate and there is zero learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.B Solving duplicate entries\n",
    "To solve the aforementioned issue, we can incorporate different approaches.\n",
    "Depending on distance metric, if we can ensure that two same object have zero distance then\n",
    "1. After constructing NxN\n",
    "matrix of N entries of our dataset, we can simply remove all objects with index `j` with zero distance for every row\n",
    "`i`. This is slow but robust appraoch as we ensure all data is cleansed.\n",
    "2. This approach works as post processesing and is faster than previous one. In this method when we want to return\n",
    "the top `K` results of for a specific input, we do not return `K` objects as there may be duplicate objects. We first\n",
    " find these objects by couting zero distance values, let's say `M` then returning `K+M` top results omitting index of\n",
    "  items included by `M`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4 Comparison of *PCA and SVD* vs. *Aggregation based* dimentionality reduction methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Methods such as PCA are statistical approaches where they use informations such as variance to extract most useful\n",
    "attributes of the given dataset. In case of PCA, in simple terms, it first looks for the dimension with most of\n",
    "variation in original data and creates a covariance matrix of these variations. Then a transformation is applied to\n",
    "the original dimensions to achieve almost same representation but with much less data. So, the goal of PCA or SVD is\n",
    "preserve the original varations of data meanwhile reducing number of attributes as much as possible.\n",
    "\n",
    "In aggregation methods, the goal is the reduce variation to have more consistent and general (stable?) interpretation\n",
    " of the original data to extract more generalized rules. For instance, combining cities into regions which will tend\n",
    " to coarse info but more consistent inter or intra region wise.\n",
    "\n",
    " Another point that should be mentioned is that PCA or SVD has defined form of statistical measurements meanwhile\n",
    " aggregation methods are some heuristics that may not be applicable task to task or need to be defined for different\n",
    " set of data/tasks/algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5 Pros and cons of sampling? Sampling without replacement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For many tasks the pure obtained data are too big and time consuming to be processed. So, to reduce the costs of\n",
    "experiment and required time we prefer to sample a small subset of original data.\n",
    "\n",
    "Sampling enables us to have almost all available information in original data with much fewer number of objects. This\n",
    " leads to huge speed up and much less expenses. But the problem is how we are going to choose a sample that could be\n",
    " representive of our entire data. Sampling means dopping most of data which leads to loss of information where maybe\n",
    " drasticly disastrous if dropped samples contain rare but desired information (think the case of anomaly detection).\n",
    "\n",
    " Simply put, a trade of between speed and quality of data is the real challenge.\n",
    "\n",
    "Sampling w/ replacement vs wo/ replacement:\n",
    "\n",
    "First of all, sampling with replacement means each time we try to sample an object, we give equal probability same as\n",
    " the intial probability of all objects but in case of without replacement, after an object got selected, we remove it\n",
    "  from the pool or make probability of it getting selected equal to zero.\n",
    "\n",
    "  What happens in the first case is that the sampled population has covariance of zero which means samples are in the\n",
    "   vicinity of their expected value. But in the case of without replacement, a data with standard deviation of sigma\n",
    "   will have a non-zero covaraince due to biased more to the last items. But if we sample infinitely from a\n",
    "   population without replacement, we will have covariance of zero because of the law of large numbers (Bernouli LLNs).\n",
    "\n",
    "   What we are looking for is a sampling with lower variance from the original dataset. Sampling with replacement\n",
    "   achieve same expectation of original dataset meanwhile sampling without replacement may exactly sample the\n",
    "   original dataset in the way that leads to same mean and zero variance for the corresponding distibution. So in\n",
    "   real world, we looking for learning new things so sampling with replacement is not good approach in machine\n",
    "   learning tasks as it may lead to biased learning or failing completely in imbalanced learning challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6 [Telco Churn dataset](https://www.kaggle.com/blastchar/telco-customer-churn) Apriori\n",
    "1. Analyze dataset\n",
    "2. Implement *Apriori*\n",
    "3. Find proper parameteres\n",
    "4. As the output, report finest extract rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7 Train *KNN* and *DecisionTree* on [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)\n",
    "1. DecisionTree\n",
    "    1. Preprocessing\n",
    "    2. Split into Train, test and Validation\n",
    "    3. Train models    \n",
    "    4. Report accuracy, recall and F-score metrics\n",
    "2. KNN\n",
    "    1. Preprocessing\n",
    "    2. Feature Extraction\n",
    "    3. Split into Train, test and Validation\n",
    "    4. Train models    \n",
    "    5. Report accuracy, recall and F-score metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 7.A DecisionTree\n",
    "1. Preprocessing\n",
    "2. Split into Train, test and Validation\n",
    "3. Train models    \n",
    "4. Report accuracy, recall and F-score metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 7.A.a Preprocessing\n",
    "In this step we apply normialization to have images with mean zero and variance of 1. This enables algorithms with\n",
    "distance based metrics to apply same importance to every pixel of image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "1DBL6E-b7wDS",
    "outputId": "9e6b3baa-ef1f-49d7-c0b0-251475130698"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train samples= 60000\n",
      "number of test samples= 10000\n",
      "shape of samples= (784,)\n"
     ]
    }
   ],
   "source": [
    "from utils import mnist_reader\n",
    "x_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "x_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "\n",
    "print('number of train samples=', len(x_train))\n",
    "print('number of test samples=', len(x_test))\n",
    "print('shape of samples=', x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "o8Rs5qk_8Ecf"
   },
   "outputs": [],
   "source": [
    "# standardization\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "# normalization\n",
    "x_train = (x_train - x_train.mean()) / x_train.var()\n",
    "x_test = (x_test - x_test.mean()) / x_test.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### 7.A.b Split into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "fGkjSAUG9WhW"
   },
   "outputs": [],
   "source": [
    "# split into train validation for hyper parameter tuning\n",
    "import random\n",
    "\n",
    "val_size = 10000\n",
    "val_index = random.sample(range(0, len(x_train)), val_size)\n",
    "split_index = np.array([0 if i in val_index else -1 for i in range(len(x_train))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### 7.A.c Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "colab_type": "code",
    "id": "q5bzAMSO-4bz",
    "outputId": "4c0cac6b-7a9e-46b0-ab07-727b7cd22718"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
       "             error_score=nan,\n",
       "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort='deprecated',\n",
       "                                              random_state=None,\n",
       "                                              splitter='best'),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'criterion': ('gini', 'entropy'),\n",
       "                         'max_depth': [10, None], 'min_samples_leaf': [2, 10]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search model training\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'criterion':('gini', 'entropy'), 'max_depth':[5, 10, None], 'min_samples_leaf':[2, 5, 10]}\n",
    "dt = DecisionTreeClassifier()\n",
    "pds = PredefinedSplit(test_fold=split_index)\n",
    "\n",
    "classifier = GridSearchCV(dt, parameters, cv=pds, scoring='accuracy')\n",
    "\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "_Jc11o3rQ9on",
    "outputId": "4db08992-0bac-47fb-bbe5-808df8e536ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 2}"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### 7.A.d Report metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "7V4cBiQfRB_D",
    "outputId": "724b2c3f-54c2-47da-9c0b-d657e9e5c146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1=0.8094858144584004, Recall=0.8114000000000001, Accuracy=0.8114\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "y_pred = classifier.predict(x_test)\n",
    "print('F1={}, Recall={}, Accuracy={}'.format(f1_score(y_test, y_pred, average='macro'), \n",
    "                                             recall_score(y_test, y_pred, average='macro'), \n",
    "                                             accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 7.B KNN\n",
    "1. Preprocessing\n",
    "2. Feature Extraction\n",
    "3. Split into Train, test and Validation\n",
    "4. Train models    \n",
    "5. Report accuracy, recall and F-score metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### 7.B.a Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### 7.B.b Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### 7.B.c Split into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### 7.B.d Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### 7.B.e Report metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
