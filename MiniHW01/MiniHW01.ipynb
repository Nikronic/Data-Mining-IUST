{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# MiniHW01 - Data Mining\n",
    "### _**Mohammad Doosti Lakhani - 98722278**_\n",
    "\n",
    "This notebook consists of comparison of different model accuracy measurements:\n",
    "\n",
    "1. Definitions\n",
    "    0. Confusion Matrix\n",
    "    1. Accuracy\n",
    "    2. Precision\n",
    "    3. Recall\n",
    "    4. F1\n",
    "    5. ROC Graph\n",
    "    6. AUC Score\n",
    "2. ROC vs F-measure\n",
    "3. Relation Between Alpha and Beta in F-measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 1. Definitions\n",
    "0. Confusion Matrix\n",
    "1. Accuracy\n",
    "2. Precision\n",
    "3. Recall\n",
    "4. F1\n",
    "5. ROC Graph\n",
    "6. AUC Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n",
    "### 1.0 Confusion Matrix\n",
    "Becaus all other metircs can be calculated, we first introduce confusion matrix.\n",
    "\n",
    "Our confusion matrix is like this image:\n",
    "\n",
    "![confusion matrix](wiki/cm.jpg)\n",
    "\n",
    "Where:\n",
    "\n",
    "* `TN = True Negative`\n",
    "* `FP = False Positive`\n",
    "* `FN = False Negative`\n",
    "* `TP = True Positive`\n",
    "\n",
    "### 1.1 Accuracy\n",
    "Accuracy is how our model can say class 1 is 1 and 0 is 0 regarding all given examples.\n",
    "\n",
    "Give confusion matrix,\n",
    "\n",
    "`Accuracy = (TN+TP) / (TN+FP+FN+TP)`\n",
    "\n",
    "So obviously more is better.\n",
    "\n",
    "### 1.2 Precision\n",
    "Based on confusion matrix,\n",
    "\n",
    "`precision = TP / (FP+TP)`\n",
    "\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. High precision relates to the low false positive rate. In our case, it says between all those we labeled as positive, how many of the are actually positive.\n",
    "\n",
    "### 1.3 Recall\n",
    "Based on confusion matrix,\n",
    "\n",
    "`recall = TP / (TP+FN)`\n",
    "\n",
    "Recall is the ratio of correctly predicted positive observations to the all observations in actual positive class.\n",
    "In our case, Between all label is positive, how many we said positive.\n",
    "\n",
    "### 1.4 F1 Score\n",
    "Based on available values,\n",
    "\n",
    "`F1 = 2*(Recall * Precision) / (Recall + Precision)`\n",
    "F1 score is weighted average of precision and recall, so it is very usefull when the cost of error of classes are different from each other (say disease detection).\n",
    "\n",
    "Here is a graph to alleviate problem:\n",
    "\n",
    "![f1 by threshold graph](wiki/f1_by_thres.png)\n",
    "\n",
    "### 1.5 ROC-AUC\n",
    "Let's talk about ROC using AUC value as these are seamlessly interchangable.\n",
    "This graph may help us to understand it better.\n",
    "\n",
    "![roc auc graph](wiki/roc_auc_curve.png)\n",
    "\n",
    "AUC is the area under ROC graph. More AUC says our model is more powerful to distinguish between the two classes. AUC = 0.5 says our model almost failed to differentiate. AUC = 1 is perfect model.\n",
    "\n",
    "For plotting we need two values:\n",
    "1. `TPR = TP / (TP + FN)`\n",
    "2. `FPR = FP / (FP + TN)` \n",
    "\n",
    "Now we compare these values regarding different classification thresholds. Increasing threshold label more entries as positive so it increases both `FP` and `TP`.\n",
    "AUC is the aggregated measure of performance among all possible thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 2 F-measure vs. ROC\n",
    "Simply put, F-measure enable us to give importance to recall or precision based on our needs. It is important to remember that F1 score is calculated from Precision and Recall which, in turn, are calculated on the predicted classes (not prediction scores)\n",
    "\n",
    "Another point that need to be mentioned is that F-measure focuses on positive class which enable us to have a clear understanding of the model's behavior regarding specific problem as the numbers are easily interpreted.\n",
    "\n",
    "The challenge in F-measure is to find the best threshold (beta) value to enable approximately best measure over recall and precision. Otherwise the interpretation may be baised toward particular metric. This also is a advantage where we can use it in imbalance datasets to focus on the anomaly (or rare case) rather than averaging over all thresholds which is what ROC does.\n",
    "\n",
    "It is a chart that visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart. Of course, the higher TPR and the lower FPR is for each threshold the better and so classifiers that have curves that are more top-left-side are better. The best ROC is the rectangle which has perfect discrimination power for any possible threshold.\n",
    "\n",
    "One case of imbalance data, ROC is not a good measure as it averages over all thresholds as this will eliminate the effect of anomaly (rare) class. An interpretation for this regarding ROC curve would be false positive rate for highly imbalanced datasets is pulled down due to a large number of true negatives. As ROC is a measure between true negative and true positive, we care about this values more, ROC is the a better measure as only focuses on this rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 3 Relation Between Alpha and Beta in F-measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![alpha beta relation in f-measure](wiki/abf.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
